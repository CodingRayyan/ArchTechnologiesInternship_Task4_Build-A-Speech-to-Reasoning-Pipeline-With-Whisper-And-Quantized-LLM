# -*- coding: utf-8 -*-
"""TArchTechnologies - Task 4 - Speech-to-Reasoning Pipeline (Whisper + 4-bit LLM).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fUpX-KPtpsynHo8VCEZObig1iyUaN4W6
"""

!pip -q install -U openai-whisper transformers accelerate bitsandbytes sentencepiece unsloth

import torch, gc, time, json, math, os, textwrap
from typing import List, Dict

print("PyTorch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    free, total = torch.cuda.mem_get_info()
    print("GPU:", device_name)
    print(f"VRAM free/total: {free/1e9:.2f} GB / {total/1e9:.2f} GB")
else:
    print("⚠️ No GPU detected. Switch to a GPU runtime.")

import whisper
from pathlib import Path
from IPython.display import Audio, display

WHISPER_MODEL_SIZE = "small"
asr_model = whisper.load_model(WHISPER_MODEL_SIZE)
print(f"Loaded Whisper: {WHISPER_MODEL_SIZE}")

def transcribe_audio(path: str) -> Dict:
    """
    Transcribe one audio file with Whisper.
    Returns: {path, text, language, segments, duration}
    """
    print(f"Transcribing: {path}")
    result = asr_model.transcribe(path, fp16=torch.cuda.is_available())
    return {
        "path": path,
        "text": result.get("text","").strip(),
        "language": result.get("language","unknown"),
        "segments": result.get("segments", []),
        "duration": result.get("duration", None),
    }

def transcribe_batch(paths: List[str]) -> List[Dict]:
    """
    Simple batch wrapper (sequential per file, uniform return structure).
    """
    return [transcribe_audio(p) for p in paths]

try:
    from google.colab import files
    uploaded = files.upload()
    AUDIO_FILES = list(uploaded.keys())
except Exception as e:
    print("Colab uploader unavailable. Set AUDIO_FILES manually.")
    AUDIO_FILES = []

AUDIO_FILES

from unsloth import FastLanguageModel

BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"
MAX_SEQ_LEN = 4096

print("Loading 4-bit model via Unsloth (bnb-4bit)...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=MAX_SEQ_LEN,
    load_in_4bit=True,
    dtype=None,
    use_gradient_checkpointing=False,
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

GEN_KW = dict(
    max_new_tokens=384,
    temperature=0.2,
    top_p=0.95,
    do_sample=True,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
print("Model ready.")

SYSTEM_PROMPT = (
    "You are a careful reasoning assistant. "
    "Explain your thinking step by step, then provide a concise final answer."
)

def build_messages(user_text: str):
    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_text},
    ]

def batch_reason(prompts: List[str], batch_size: int = 2) -> List[str]:
    """
    Tokenize + pad in batches to reduce VRAM spikes.
    """
    import torch, gc
    model.eval()
    outputs = []
    for i in range(0, len(prompts), batch_size):
        chunk = prompts[i:i+batch_size]
        if hasattr(tokenizer, "apply_chat_template"):
            texts = [
                tokenizer.apply_chat_template(build_messages(p), tokenize=False, add_generation_prompt=True)
                for p in chunk
            ]
        else:
            texts = [f"<|system|>\n{SYSTEM_PROMPT}\n<|user|>\n{p}\n<|assistant|>\n" for p in chunk]

        enc = tokenizer(
            texts, return_tensors="pt", padding=True, truncation=True, max_length=2048
        )
        if torch.cuda.is_available():
            enc = {k: v.cuda() for k, v in enc.items()}

        with torch.no_grad():
            out_tokens = model.generate(**enc, **GEN_KW)

        for j in range(len(chunk)):
            input_len = enc["input_ids"][j].shape[0]
            full = out_tokens[j]
            gen = full[input_len:]
            outputs.append(tokenizer.decode(gen, skip_special_tokens=True))

        del enc, out_tokens
        torch.cuda.empty_cache(); gc.collect()
    return outputs

from transformers import pipeline

audio_file = "Surah Al Baqarah Last 2 Ayaat  Last 2 Verses Of Surah Al Baqarah  Surah Baqarah ki Aakhri 2 Ayat.mp3"
asr_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-small")

asr_result = asr_pipeline(audio_file, return_timestamps=True)

asr_outputs = [asr_result]

print("Transcription:", asr_outputs)

def make_reasoning_prompt(tr: dict) -> str:
    t = tr.get("text", "").strip()
    lang = tr.get("language", "unknown")
    duration = tr.get("duration", None)
    meta = f"(language={lang}, duration={duration}s)"
    return (
        f"You are given a spoken query {meta}:\n\n"
        f"\"\"\"\n{t}\n\"\"\"\n\n"
        f"1) Understand the intent.\n"
        f"2) If it is a question, answer it with reasoning.\n"
        f"3) If it is a command, outline clear steps.\n"
        f"4) End with a one-line TL;DR."
    )

prompts = [make_reasoning_prompt(x) for x in asr_outputs]

for i, p in enumerate(prompts, 1):
    print(f"\n--- Prompt {i} ---\n{p[:600]}{'...' if len(p)>600 else ''}")

if prompts:
    t0 = time.time()
    responses = batch_reason(prompts, batch_size=2)   # set to 1 if you hit OOM
    print(f"\nGenerated {len(responses)} response(s) in {time.time()-t0:.1f}s\n")
    for i, (tr, resp) in enumerate(zip(asr_outputs, responses), 1):
        print(f"\n=== Audio {i}: {audio_file} ===")   # ✅ replaced tr['path'] with filename
        print("TRANSCRIPT:", tr['text'][:300] + ("..." if len(tr['text']) > 300 else ""))
        print("\nASSISTANT:\n", resp.strip())
else:
    print("Upload audio(s) and run transcription first.")

def transcribe_then_reason(audio_paths: List[str]) -> List[Dict]:
    trs = transcribe_batch(audio_paths)
    prs = [make_reasoning_prompt(x) for x in trs]
    ans = batch_reason(prs, batch_size=2)
    return [{"path": t["path"], "text": t["text"], "answer": a} for t, a in zip(trs, ans)]